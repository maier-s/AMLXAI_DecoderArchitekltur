\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{IEEEtran}
\citation{vaswaniAttentionAllYou2023}
\citation{yangHarnessingPowerLLMs}
\citation{radfordImprovingLanguageUnderstanding}
\citation{vaswaniAttentionAllYou2023}
\citation{radfordImprovingLanguageUnderstanding}
\citation{WieTransformatorenFunktionieren}
\citation{WieTransformatorenFunktionieren}
\@writefile{toc}{\contentsline {section}{\numberline {I}Einführung}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {I-A}}Ziel der Arbeit}{1}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {I-B}}Aufbau und Struktur der Arbeit}{1}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {II}Decoder Architektur}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-A}}Historische Entwicklung der GPT-Decoder Architektur}{1}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-B}}Aufbau der GPT-Decoder Architektur}{1}{subsection.2.2}\protected@file@percent }
\citation{WieTransformatorenFunktionieren}
\citation{vaswaniAttentionAllYou2023}
\citation{WieTransformatorenFunktionieren}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Historische Entwicklung von Transformer Modellen aus \cite  {yangHarnessingPowerLLMs}}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:Historie}{{1}{2}{Historische Entwicklung von Transformer Modellen aus \cite {yangHarnessingPowerLLMs}}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Decoder Architektur nach \cite  {radfordImprovingLanguageUnderstanding}}}{2}{figure.2}\protected@file@percent }
\newlabel{fig:DecoderArchitektur}{{2}{2}{Decoder Architektur nach \cite {radfordImprovingLanguageUnderstanding}}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Multi-Self Attention Layer nach \cite  {WieTransformatorenFunktionieren}}}{2}{figure.3}\protected@file@percent }
\newlabel{fig:MultiSelfAttention}{{3}{2}{Multi-Self Attention Layer nach \cite {WieTransformatorenFunktionieren}}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Attention Mechanismus nach \cite  {WieTransformatorenFunktionieren}}}{2}{figure.4}\protected@file@percent }
\newlabel{fig:AttentionMechanismus}{{4}{2}{Attention Mechanismus nach \cite {WieTransformatorenFunktionieren}}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Maskierung der Attention Weights nach \cite  {WieTransformatorenFunktionieren}}}{2}{figure.5}\protected@file@percent }
\newlabel{fig:Maskierung}{{5}{2}{Maskierung der Attention Weights nach \cite {WieTransformatorenFunktionieren}}{figure.5}{}}
\citation{radfordImprovingLanguageUnderstanding}
\citation{radfordImprovingLanguageUnderstanding}
\citation{vaswaniAttentionAllYou2023}
\citation{radfordImprovingLanguageUnderstanding}
\citation{radfordImprovingLanguageUnderstanding}
\citation{radfordImprovingLanguageUnderstanding}
\citation{radfordImprovingLanguageUnderstanding}
\bibdata{AMLundXAI}
\bibcite{vaswaniAttentionAllYou2023}{1}
\bibcite{yangHarnessingPowerLLMs}{2}
\bibcite{radfordImprovingLanguageUnderstanding}{3}
\bibcite{WieTransformatorenFunktionieren}{4}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-C}}Abgrenzung der Decoder Architektur zur Encoder Architektur}{3}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-D}}Trainingsprozess der GPT-Decoder Architektur}{3}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\numberline {\mbox  {II-D}0a}unüberwachtes Vortrainieren}{3}{paragraph.2.4.0.1}\protected@file@percent }
\newlabel{eq:Zielfunktion1}{{1}{3}{unüberwachtes Vortrainieren}{equation.2.1}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {\mbox  {II-D}0b}überwachtes Fine-Tuning}{3}{paragraph.2.4.0.2}\protected@file@percent }
\newlabel{eq:Zielfunktion2}{{2}{3}{überwachtes Fine-Tuning}{equation.2.2}{}}
\newlabel{eq:Zielfunktion3}{{3}{3}{überwachtes Fine-Tuning}{equation.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Anpassung der Input Tokens nach \cite  {radfordImprovingLanguageUnderstanding}}}{3}{figure.6}\protected@file@percent }
\newlabel{fig:InputTokens}{{6}{3}{Anpassung der Input Tokens nach \cite {radfordImprovingLanguageUnderstanding}}{figure.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Anwendungsbereiche}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {IV}Potenzial und Grenzen von Decoder Architekturen}{3}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {V}Fazit}{3}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-A}}Aktuelle Forschungbereiche}{3}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-B}}Ausblick}{3}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{References}{3}{section*.1}\protected@file@percent }
\gdef \@abspage@last{3}
