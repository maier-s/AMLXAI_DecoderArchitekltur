\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Transformer Decoder Architektur}

\author{\IEEEauthorblockN{1\textsuperscript{st} Stefan Maier}
\IEEEauthorblockA{\textit{Function and Algorithm} \\
\textit{ZF Lifetec}\\
Alfdorf, Deutschland \\
Stefan.Maier1@zf-lifetec.com}

}

\maketitle
%Literatur aus AMLundXAI.bib einfügen mit 
\bibliographystyle{IEEEtran}



\begin{abstract}
Große Sprache Modelle (engl. Large Language Models) wie Chat GPT basieren auf der Transformer Architektur neuronaler Netze. Die Transformer Architektur besteht dabei aus einem Encoder- sowie Decoder-Element. In der Wissenschaft und Forschung existieren auch Architekturen welche nur einen Teil eines Transformers zur Lösung eines Problems verwenden. In dieser Arbeit wird genauer auf Transformer Architekturen eingegange, welche lediglich aus dem Decoder Element bestehen. Eine weit Verbreitete Decoder Architektur ist die von Chat GPT verwendete GPT (kurz: Generative Pretrained Transformer) Architektur. Es wird auf die Architektur und die weiter Entwicklung der Architektur eingegangen.
\end{abstract}

\begin{IEEEkeywords}
component, formatting, style, styling, insert.
\end{IEEEkeywords}

\section{Einführung}
Die Verarbeitung von natürlicher Sprache (engl. Natural Language Processing. kurz NLP) ist ein wichtiger und großer Bestandteil der künstlichen Intelligenz Forschung. Die Forschungsbereiche decken dabei ein breites Spektrum an Aufgaben ab, wie z.B. Beantwortung von Fragen, Semantische Ähnlichkeit, Text Generierung, Dokumenten Klassifikation o.ä. Die ersten Fortschritte wurde im NLP Bereich bereits durch statistische Hidden Markov Modelle erzielt. Die Leistung solcher statistischen Modelle ist jedoch begrenzt und konnte die Komplexität der natürlichen Sprache nicht geeignet abbilden. Mit den Entwicklungen im Bereich des Deep-Learnings gelangen entscheidende Schritte in der NLP Forschung. Die Transformer Architektur, und das damit verbundene vortrainieren großer Sprachmodelle wie BERT, GPT,T5 oder RoBERTa haben große Fortschritte gebracht, die sich nicht nur auf die NLP Forschung beschränken sondern in der allgemeinen Gesellschaft und Wirtschaft Einzug halten.

\subsection{Ziel der Arbeit}
Das Ziel der Arbeit ist es die Decoder Architektur welche Häufig von generativen Sprachmodellen verwendet wird genauer zu beleuchten. Dabei wird die Decoder-only Architektur anhand des Generative Pretrained Transformers erklärt. Neben dem Generative Pretrained Transformer existieren ebenfalls weitere Modelle die kurz erläutert werden. Weiterhin soll die Arbeit die Anwendungsgebiete sowie Potenziale und Grenzen der Decoder Architektur aufzeigen. Im Ausblick wird auf aktuelle Forschungsfelder im Bereich der Decoder Architekturen eingegangen.
\subsection{Aufbau und Struktur der Arbeit}
Die Arbeit ist wie folgt strukturiert: In Kapitel 2 wird zunächst auf die Grundlagen in Form der Decoder Architektur als Teil der Transformer Architektur eingegangen. Dazu wird die historische Entwicklung, der Aufbau und der Trainingsprozess der Decoder Architektur erläutert und diese vom Encoder Teil der Transformer Architektur abgegrenzt. In Kapitel 3 wird auf unterschiedliche mögliche Anwendungsgebiete aus der Forschung und Entwicklung eingegangen. Aus diesen Erkenntnissen werden in Kapitel 4 die Potenziale und damit einhergehenden Grenzen solcher großen Sprachmodelle eingegangen. Kapitel 5 fasst die Ergebnisse zusammen und gibt einen Ausblick auf die aktuellen Forschungsbereiche.
\section{Decoder Architektur}
Im folgenden Abschnitt wird die Decoder Architektur am Beispiel des Generative Pretrained Transformers (GPT) erläutert. Dabei wird auf die historische Entwicklung, den Aufbau, den Trainingsprozess und die Abgrenzung zur Encoder Architektur eingegangen.
\subsection{Historische Entwicklung der GPT-Decoder Architektur}
Transformer bilden das Grundgerüst moderne großer Sprachmodelle. Diese wurden 2017 in \cite{vaswaniAttentionAllYou2023}  beschrieben. 2018 wurde die erste Version von GPT veröffentlicht, welche auf dem Decoder Part der Transformer Architektur aufbaut. Auf Basis der GPT Modell entstanden weitere Decoder-only Modelle welche die folgende \autoref{fig:Historie} gut veranschaulicht.
\begin{figure}[htbp]
    \centerline{\includegraphics[width=\linewidth]{Bilder/EntwicklungLLM.png}}
    \caption{Historische Entwicklung von Transformer Modellen aus \cite{yangHarnessingPowerLLMs}}
    \label{fig:Historie}
\end{figure}
Die \autoref{fig:Historie} zeigt die Entwicklung unterschiedlicher Transformer Modell bis zum letzten Jahr. Der Zweig der Decoder Only Architektur stellt den Entwicklungszweig reiner Decoder Architekturen dar . Mit GPT wurde das erste rein auf der Decoder Struktur basierende Neuronale Netzwerk entworfen. Daraus entsprangen viele unterschiedliche Decoder-Only Modelle von Google, Meta oder anderen Forschungseinrichtungen. Die aktuellsten Versionen von Llama, GPT-4 oder Bard stellen dabei bis heute die neusten Entwicklungen dar.
\subsection{Aufbau der GPT-Decoder Architektur}
Wie im vorherigen Abschnit beschrieben legt das GPT Modell den Grundstein für die Decoder-only Architektur. Dies wurde durch das Paper \cite{radfordImprovingLanguageUnderstanding} beschrieben und basiert dabei auf den Ideen der in \cite{vaswaniAttentionAllYou2023} beschrieben Transformer Architekturen. Die 
folgende \autoref{fig:DecoderArchitektur} illustriert dabei den Aufbau einer Decoder Schicht des GPT Modells.
\begin{figure}[htbp]
    \centerline{\includegraphics[width=0.4\linewidth]{Bilder/DecoderArchitekturGPT.png}}
    \caption{Decoder Architektur nach \cite{radfordImprovingLanguageUnderstanding}}
    \label{fig:DecoderArchitektur}
\end{figure}
Die \autoref{fig:DecoderArchitektur} zeigt dabei die Elemente einer Decoder Schicht. Die Elemente werden logisch zu einer Decoder Schicht (blau) zusammengefasst. Eine Decoder Schicht besteht dabei aus einem Masked Multi-Self Attention Layer, Feed Forward Netzwerken, sowie Layer Normalisierungs Schicht um  Residuen Verbindungen der vorherigen Schicht hinzuzufügen. Die Residuen lösen das Problem der verschwindend Gradienten beheben \cite{WieTransformatorenFunktionieren}. Als Eingabe für die Decoder Schicht dienen dabei immer entsprechende Embeddings mit Positionskodierung, welche die Position der Embeddings in einem Satz bestimmen \cite{WieTransformatorenFunktionieren}.
Der Kernbestandteil der Decoder Schicht ist dabei das Masked Multi-Self Attention Layer. Diese Schicht ist eine leichte Abwandlung des Mult Self Attention Layers, welches auch in Encoder Elementen eines Transformers verwendet wird. Nachfolgende \autoref{fig:MultiSelfAttention} zeigt den Aufbau eines Multi-Self Attention Layers.
% Neues Bild
\begin{figure}[htbp]
    \centerline{\includegraphics[width=\linewidth]{Bilder/MultiheadSelfAttention.png}}
    \caption{Multi-Self Attention Layer nach \cite{WieTransformatorenFunktionieren}}
\label{fig:MultiSelfAttention}
\end{figure}
Als Eingabe werden die Embeddings der vorherigen Schicht verwendet und in Query, Key und Value aufgeteilt. Query ist dabei der der Text der verarbeitet wird, Key der Referenz Text gegen den verglichen wird und Value wird für die Ausgabeberechnung des Attention Elements verwendet.
In \autoref{fig:MultiSelfAttention} wird der Attention Mechanismus (dargestellt links) $h$ mal parallel berechnet (dargestellt rechts). Daher wird dieses Modellelement Multi Self Attention bezeichnet. 
Zur Berechnung der Ausgabe des Attention Layers werden die Attention Gewichte zunächst berechnet welche bestimmen, wie wichtig ein Wort für ein anderes Wort ist, was durch nachfolgende \autoref{fig:AttentionMechanismus} visualisiert ist. Die Attention Gewichte werden dabei durch die Matrix Multiplikation von Query und Key erzeugt. Query und Key durchlaufen dazu zunächst ein Lineares Layer dessen Gewichte erlernt werden müssen.
% Bild AttentionMechanismus
\begin{figure}[htbp]
    \centerline{\includegraphics[width=\linewidth]{Bilder/AttentionMechanismus.png}}
    \caption{Attention Mechanismus nach \cite{WieTransformatorenFunktionieren}}
\label{fig:AttentionMechanismus}
\end{figure}
Die Färbung in der \autoref{fig:AttentionMechanismus} zeigt dabei die Wichtigkeit der Wörter zueinander. Die daraus berechnete Attention Gewichte werden mit dem Werten der Wörter multipliziert (zweites MatMult) und über ein Lineares Layer verfeinert, dies ergibt die Ausgabe des Attention Layers. 
Der wesentliche Unterschied zwischen dem Multi-Self Attention Layer und dem Masked Multi-Self Attention Layer ist dabei die Maskierung der Attention Gewichte. Die Maskierung sorgt dafür, dass ein Wort nur von vorherigen Wörtern abhängt. Die nachfolgende \autoref{fig:Maskierung} zeigt die Maskierung der Attention Gewichte mithilfe einer Maskierungsmatrix. Die resultierende Matrix enthält für alle noch unbekannten Tokens eine $-\infty$ und für alle bekannten Tokens den Wert der skalierten Attention Gewichte.
% Bild Maskierung

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\linewidth]{Bilder/MaskierungAttentionWeights.png}}
    \caption{Maskierung der Attention Gewichte nach \cite{WieTransformatorenFunktionieren}}
\label{fig:Maskierung}
\end{figure}
Die genaue Funktionsweise der einzelnen Schichten aus \autoref{fig:DecoderArchitektur}  wird in \cite{vaswaniAttentionAllYou2023} beschrieben. Abschließend wird in der GPT Architektur ein weiteres Layer je nach Aufgabe angehängt, welche in \autoref{fig:DecoderArchitektur} durch Text Vorhersage oder Aufgaben Klassifizierung repräsentiert wird. Dies kann beispielsweise ein Lineares Layer mit Softmax Aktivierungsfunktion sein, welches die Wahrscheinlichkeiten für das nächste Wort aus dem Vokabular berechnet \cite{WieTransformatorenFunktionieren}.
\subsection{Abgrenzung der Decoder Architektur zur Encoder Architektur}
Die vorgeschlagene Decoder Architektur durch \cite{radfordImprovingLanguageUnderstanding} unterscheidet sich dabei von der Encoder Architektur durch die Verwendung von Masked Multi-Self Attention Layern. Die Encoder Architektur verwendet hingegen Multi-Self Attention Layer ohne Maskierung. Entscheidend ist ebenfalls das die Architektur aus \cite{radfordImprovingLanguageUnderstanding} keine Einbeziehung der Encoder Schicht vorsieht und somit im Vergleich zur Transformer Architektur aus \cite{vaswaniAttentionAllYou2023} keine weitere Multi-Self Attention Layer enthält. Die Decoder Architektur ist dabei auf die Generierung von Text ausgelegt und wird als autoregressives Modell betrachtet. Die Encoder Architektur hingegen ist auf die Verarbeitung von Texten ausgelegt und wird als diskriminatives Modell betrachtet.
\subsection{Trainingsprozess der GPT-Decoder Architektur}
Das entscheidende bei der in \cite{radfordImprovingLanguageUnderstanding} beschrieben Decoder Architektur ist der Trainingsprozess der aus zwei Stufen besteht.
\subsubsection{unüberwachtes Vortrainieren}
Das unüberwachte Vortrainieren des Modells. Dabei ist die Zielfunktion folgende Likelyhoodfunktion:
\begin{equation}
    {\cal L}_1({\cal U}) = \sum_{i}^{} log P( u_i|u_i-k ... u_i-1;\theta)
    \label{eq:Zielfunktion1}
\end{equation}
$k$ ist dabei das Kontext Fenster. Somit wird die Wahrscheinlichkeit des nächsten Wortes auf Basis der vorherigen $k$-Tokens bestimmt. Das erste GPT Modell wurde dabei auf Tokens aus dem BookCorpus Datensatz trainiert. Wie bereits in \autoref{fig:DecoderArchitektur} beschrieben wurden 12 Decoder Schichten verwendet. Für die Attention Heads $h$ wurden 12 Heads verwendet bei 768 Embedding Dimensionen. Für das Feed Forward Netzwerk wurde eine GELU (Gaussian Error Linear Unit) verwendet. Das Training wurde über 100 Epochen mit einer Batchgröße von 64 durchgeführt.
\subsubsection{überwachtes Fine-Tuning}
Der zweite Schritt ist das überwachte Fine-Tuning auf die gewünschte Zielaufgabe. Dabei handelt es sich um $m$-Eingabe Tokens, welche in eine Klasse aus $\cal C$ transformiert werden. Die Zielfunktion ist folgende Likelyhoodfunktion:
\begin{equation}
    {\cal L}_2({\cal C}) = \sum_{(x,y)}^{} log P(y|x^1 ... x^m)
    \label{eq:Zielfunktion2}
\end{equation}
Dabei wird die Zielfunktion aus \autoref{eq:Zielfunktion2} mit der Funktion aus \autoref{eq:Zielfunktion1} zu \autoref{eq:Zielfunktion3} kombiniert damit der Trainingsprozess besser konvergiert und generalisiert \cite{radfordImprovingLanguageUnderstanding}.
\begin{equation}
    {\cal L}_3({\cal C}) = {\cal L}_2({\cal C}) + \lambda {\cal L}_1({\cal C})
    \label{eq:Zielfunktion3}
\end{equation}
Da das vortrainierte Modell wie in \autoref{eq:Zielfunktion1} beschrieben auf die vorhersage der nächsten Worte optimiert ist, müssen für das Finetuning die Input Tokens entsprechend der nachfolgenden \autoref{fig:InputTokens}, wie in \cite{radfordImprovingLanguageUnderstanding} beschrieben, angepasst werden.
% Bild Input Tokens
\begin{figure}[htbp]
    \centerline{\includegraphics[width=\linewidth]{Bilder/TokenAnpassungGPT1.png}}
    \caption{Anpassung der Input Tokens nach \cite{radfordImprovingLanguageUnderstanding}}
\label{fig:InputTokens}
\end{figure}
Dabei werden strukturierte Daten in eine geordnete Reihenfolge gebracht, um die Eingabe für das Modell zu generieren. \autoref{fig:InputTokens} zeigt, dass für eine Multiple Choice Aufgabe zuerst ein zufällig ausgewähltes Start Token, der Kontext und anschließend die Antwort mit einem zufällig ausgewählten Wert für das Abschluss Token übergeben wird. Für diese spezifische Aufgabe werden anschließend drei Transformer parallel trainiert, um die korrekte Antwort vorherzusagen. Für das Finetuning wurden die meisten Hyperparameter aus dem Unüberwachten Training übernommen. Lediglich wurde ein Dropout vor der Klassifizierungsschicht hinzugefügt und die Lernrate wurde reduziert. Das Paper aus \cite{radfordImprovingLanguageUnderstanding} zeigt dabei, dass 3 Epochen ausreichen um eine gewünschte Genauigkeit zu erzielen.

\subsection{Weiterentwicklung der GPT-Decoder Architektur}
Die erste GPT Version ist ein Meilenstein in der Entwicklung von Decoder-only Architekturen. Die Weiterentwicklung der GPT Architektur zeigt dabei die Entwicklung von GPT-2, GPT-3 und GPT-4. 
\subsubsection{GPT-2}
GPT-2 verbesserte die Funktionsweise von großen Sprachmodellen, dass diese mehrere Aufgaben lösen können. Dabei war das Ziel ein Modell zu entwerfen, welches eine Warscheinlichkeit für eine Output Token Sequenz in Abhängigkeit der Input Tokens sowie der Aufgaben Spezifikation zu erlernen.
\begin{equation}
    P(s_{n-k}, ..., s_n|s_1, ..., s_{n-k-1},task)
    \label{eq:Wahrscheinlichkeit}
\end{equation}
Für das Training des GPT-2 Modell wird als Datensatz WebText verwendet welche aus Inhalten kuratierter Websiten besteht. Die Modell Architektur ist ähnlich zur ursprünglichen GPT Architektur, jedoch wurde die Anzahl des Kontextfensters von 512 auf 1024 erhöht. Anstatt 12 Decoder Schichten wurden bei GPT-2 48 Schichten verwenden mit einer Embedding Dimension von 1600 was zu einer Parametermenge von ca. 1,5 Milliarden Parameter führt. GPT-2 zeigt zum ersten mal das ein großes Sprachmodell auf mehrere Aufgaben gleichzeitig implizit trainiert werden kann, sofern der Trainingsdatensatz divers genug ist \cite{radfordLanguageModelsAre}. 
\subsubsection{GPT-3}
Während der Fokus bei GPT-2 vorallem auf der Zero Shot Performance und das Unsupervised Mulittask Learning liegt, wurde das GPT-3 Modell im Fokus des Few Shot Learnings betrachtet. GPT-3 erweitert auch hier die Funktionalität von GPT-2 erneut und verwendet statt des Masked Multi-Self Attention Layer, Attention Konzepte aus \cite{childGeneratingLongSequences2019} ähnlich der sogenannten Sparse Transformes. Diese ermöglichen es Längere Sequenzen zu verarbeiten und das Training tieferer Modelle zu ermöglichen. Das Paper aus \cite{brownLanguageModelsAre2020} zeigt dass größere Modelle entscheidende Vorteile beim Meta Learning oder Few-Shot Learning haben. So besteht das größte GPT-3 Modell aus 175 Milliarden Parametern. 
\subsubsection{GPT-4}
Das neueste Modell von OpenAI GPT-4 zeigt die Weiterentwicklung von GPT-3 und setzt zusätzlich zum Unsupervised Vortraining auf Finetuning über Menschen gesteuertes Feedback mittels Reinforcement Learning. Leider gibt es zum aktuellen Zeitpunkt keine weiteren Details zur Architektur von GPT-4 wie in \cite{openaiGPT4TechnicalReport2024} beschrieben. Das Paper in \cite{openaiGPT4TechnicalReport2024} zeigt jedoch anhand der verwendeten Trainingsressourcen, dass das GPT Modell entsprechend groß ist. Zudem wird beschrieben, dass eine Infrastruktur im Rahmen der GPT4 Entwicklung entwickelt wurde um verlässliche die Entwicklung des Modells vorher zu sagen und die Trainingsressourcen zu optimieren.

\section{Anwendungsbereiche}
In den unterschiedlichen Papern werden bei der Evaluation ebenfalls unterschiedliche NLP Aufgaben betrachtet die für eine Anwendung großer Sprachmodelle in Frage kommen. Es lassen sich 10 Anwendungsbereiche auf Basis von \cite{keary12PraktischeLarge2023} identifizieren, die große Potenziale bieten.
\subsection{Übersetzung}
\cite{jiaoChatGPTGoodTranslator2023} zeigt, dass ChatGPT ein guter Übersetzer ist. Es zeigt sich jedoch auch, dass ChatGPT besser europäische Sprachen übersetzen kann als fernere Sprachen.
\subsection{Erstellung von Inhalten}
Das GPT-3 Paper \cite{brownLanguageModelsAre2020} zeigte bereits, dass dieses Nachrichten Artikel generieren kann, welche für den Menschen als schwer unterscheidbar gelten. Daher eignenen sich große Sprachmodelle auch für die Erstellung diverser Inhalte. Beispielsweise können Inhalte erstellt werden, die der Ideenfindung oder Inspiration dienlich sein können. Beispiele können hierfür Blogs, Fragebögen oder auch beiträge in sozialen Medien sein \cite{keary12PraktischeLarge2023}.
\subsection{Suchwerkzeug}
Im Anwendungsbereich der Suchwerkzeuge eignet sich ein großes Sprachmodell, wie bereits das GPT-2 Paper \cite{radfordLanguageModelsAre} zeigt. Weiterhin kann das Modell für eine Schlüsselwort Forschung (engl. Keyword Research) verwendet werden um die wichtigsten Schlüsselwörter für ein Thema zu erhalten. Damit können SEO-freundliche Inhalte erstellt werden \cite{keary12PraktischeLarge2023}.
\subsection{Virtuelle Assistenten und Kundenbetreuung}
Als Einsatz von Chatbots zeigt auch Zalando in \cite{wuttkeLargeLanguageModel2023}, dass große Sprachmodelle für die Kundenbetreuung eingesetzt werden können. Dabei können Kundenanfragen automatisiert beantwortet werden. Das Sprachmodell unterstützt dabei den potenziellen Kunden bei der Auswahl von Produkten für bestimmte Anlässe oder gibt Vorschläge auf basis der persönlichen Präferenzen und Vorlieben.
\subsection{Erkennung und Verhinderung von Cyberangriffen}
Große Sprachmodelle eignen sich unter anderem zur Erkennung von Cyberangriffen SentinalOne sowie Microsoft zeigen wie große Sprachmodelle Datensätze auf Muster hinsichtlich Cyberangriffen scannen um so frühzeitig Angriffe zu erkennen \cite{MicrosoftCopilotSecurity}\cite{SentinelOneUnveilsRevolutionary}.
Mit dem LLM SecPaLM zeigt Google, dass große Sprachmodelle verwendet werden können um das Verhalten von Skripten scannen zu können. Damit muss der Benutzer die Software nicht zwangsläufig in einer Sandbox ausführen um zu prüfen ob die Software schädlich ist \cite{keary12PraktischeLarge2023}.
\subsection{Code-Entwicklung}
Ein weiterer wichtiger Anwendungsfall ist die Unterstützung von großen Sprachmodellen bei der Code-Entwicklung. LLM's können dabei Code Abschnitte schreiben, Dokumentation generieren oder bei der Fehlersuche während des Programmierens unterstützen. \cite{keary12PraktischeLarge2023}
\subsection{Transkription}
Im Paper von GPT-4 \cite{openaiGPT4TechnicalReport2024} wird auf die Multimodalität eingegangen und gezeigt wie gut große Sprachemodelle beispielsweise Bilder verarbeiten können. Diese Fähigkeit kann dazu genutzt werden um Transkritpionen anzufertigen \cite{keary12PraktischeLarge2023}.
\subsection{Marktforschung}
In der Marktforschung müssen viele Datensätze analysiert und ausgewertet werden. So kann ein großes Sprachmodell beispielsweise bei der Marktforschung unterstützen indem es Zusammenfassungen erstellt, Trends erkennt oder Marktlücken identifitiert \cite{keary12PraktischeLarge2023}.
\subsection{Vertriebsautomatisierung}
Große Sprachmodelle können auch im Vertriebswesen verwendete werden um Vertribsprozesse zu automatisieren. \cite{keary12PraktischeLarge2023} zeigt, dass LLM's bei der Lead-Generierung, Pflege, Personalisierung, Qualifizierung, Bewertung sowie Lead Prognose unterstützen kann, indem es entsprechende Lead Datensätze auswertet.
\subsection{Stimmungsanalyse}
Ein weiterer wichtiger Anwendungsbereich ist in der Stimmungsanalyse. Große Sprachmodelle können dabei helfen die Stimmung von Texten zu analysieren und so beispielsweise die Stimmung von Kunden in Online Shops zu analysieren \cite{keary12PraktischeLarge2023}. Mit der Stimmungsanalyse können Unterhnehmen die assozierten Wörter mit dem Unterhnehmen aus Kommentaren herausfiltern und erhalten ein Bild über die Marke des Unternehmens. Abhängig davon lässt sich die Marke entsprechend der Stimmungslage verbessern \cite{keary12PraktischeLarge2023}.

\section{Potenzial und Grenzen von Decoder Architekturen}
Der vorherige Abschnitt zeigt das breite Anwendungsspektrum von Decoder Architekturen. Da sich Decoder Architekturen stetig weiter entwickeln werden sich die Anwendungsfelder ebenfalls erweitern. Nach \cite{keary12PraktischeLarge2023} wird das Marktpotenzial generativer KI's bis 2032 auf bis zu 1,3 Billionen Dollar geschätzt. Neben den Potenzialen gibt es jedoch auch Grenzen die bei der Verwendung von Decoder Architekturen beachtet werden müssen. Es lässt sich dabei unterscheiden in Potenziale und Grenzen durch die Architketurwahl selbst und durch das konkrete Modell wie bspw. GPT.  Für GPT-3 wurden die Poteziale und Grenzen in \cite{radfordLanguageModelsAre} und \cite{openaiGPT4TechnicalReport2024} genauer beschrieben.
\subsection{Potenziale und Grenzen der Decoder Architektur}
\subsubsection{Potenziale}
Nach \cite{kulkarniAppliedGenerativeAI2023} bieten dabei Transformer Architekturen mit Self Attention Mechanismus folgende Vorteile:
\begin{itemize}
    \item Parallelisierung und Effizienz $\rightarrow$ Der Self Attention Mechanismus verarbeitet parallel die Eingabe Tokens.
    \item Langfristige Abhängigkeiten $\rightarrow$ Der Self Attention Mechanismus bietet im Vergleich zu Rekursiven Neuronalen Netzwerken die Möglichkeit lange Abhängigkeiten zu besser zu erlernen.
    \item Skalierbarkeit $\rightarrow$ Die Komplexität für den Attention Mechanismus is Linear und somit skaliert ein Transformer Modell besser als andere Netzwerke über die Token Länge.
    \item Transfer Learning mit Transformer $\rightarrow$ Wie auch die Paper von GPT zeigen, können Transformer Modelle ohne große Architekturanpassung auf unterschiedliche Anwendungsbereiche gefinetuned werden.
    \item Kontextuelle Embeddings $\rightarrow$ Die Verwendung von Kontextualisierten Embeddings macht es möglich das ein Wort mehrere Bedeutungen in Abhängigkeit des Kontextes haben kann.
    \item Globale Informationsverarbeitung $\rightarrow$ Durch die parallele Verarbeitung aller Tokens im Attention Mechanismus können globale Informationen verarbeitet werden. Im Vergleich zu RNN's welche die Daten sequenziell verarbeiten.
\end{itemize}
\subsubsection{Grenzen}
Die Grenzen nach \cite{kulkarniAppliedGenerativeAI2023} sind dabei:
\begin{itemize}
    \item Attention Overhead bei langen Sequenzen $\rightarrow$ Für extreme Lange Sequenzen kann der Attention Mechanismus sehr Aufwendig zu berechnen sein.
    \item Fehlende Reihenfolge $\rightarrow$ Wird zwar durch das Positional Encoding im Eingangslayer erreicht, ist aber jedoch nicht so explizit wie bei RNN's.
    \item Übermäßige Parametrisierung $\rightarrow$ Große Sprachmodelle haben eine hohe Anzahl an Parametern. Damit ist das Training solcher Modelle wesentlich Aufwändinger als bei anderen Modellen.
    \item Unstrukturierte Eingabedaten $\rightarrow$ Transformer oder auch Decoder Architekturen wurden für strukturierte Daten entwickelt. Unstrukturierte Daten wie Bilder oder Audiodaten können nicht direkt verarbeitet werden. Dennoch zeigt GPT-4 die Multimodalität von LLM's \cite{openaiGPT4TechnicalReport2024}.
    \item Feste Eingabelänge $\rightarrow$ Typischer weise besitzen Transformer Modelle eine feste Eingabelänge. Dennoch gibt es Transfromer Architekturen welche mit Variablen Eingabelängen umgehen können \cite{kulkarniChatGPTArchitectureInDepth2023}. 
\end{itemize}
\subsection{Potenziale und Grenzen von GPT}
Im nachfolgenden werden die Potenziale und Grenzen von GPT nach \cite{kulkarniChatGPTArchitectureInDepth2023} beleuchtet.
\subsubsection{Potenziale}
Folgen Potenziale bietet das GPT Modell:
\begin{itemize}
    \item Verständnis des Kontexts $\rightarrow$ GPT Modelle können den Kontext eines Satzes besser verstehen und somit besser auf die Anfrage des Nutzers eingehen.
    \item Large-Scale Sprachmodell $\rightarrow$ Die GPT Mdelle sind auf Texten aus dem Internet vortrainiert und haben damit ein breite Wissensbasis.
    \item Fine-Tuning Prozess $\rightarrow$ Durch das Menschen gesteuerte Feedback Fine-tuning kann GPT bessere uns sichere Antworten generieren.
    \item Iterative Entwicklung $\rightarrow$ Durch ständige Forschung und Weiterentwicklung entsteht eine immer verbesserte Version von GPT.
\end{itemize}
\subsection{Grenzen}
Die Grenzen von GPT sind dabei:
\begin{itemize}
    \item Mangelndes Weltwissen $\rightarrow$ GPT Modelle haben nur auf Daten und Informationen zugriff auf diese das Modell trainiert worden ist. Damit Fehlt dem GPT Echtzeit Informationen.
    \item Bias im Sprachmodell $\rightarrow$ GPT Modelle können durch die Trainingsdaten Biases aufweisen. Die Biases können durch entsprechendes Fine-Tuning reduziert aber nicht komplett eliminiert werden. In den Papern von GPT-3 wird genauer auf Biases eingegangen \cite{brownLanguageModelsAre2020}.
    \item Unangemessene oder unsichere Ausgaben $\rightarrow$ GPT produziert manchmal unangemessene oder unsichere Ausgaben.
    \item Fehlen von tiefem Verständnis $\rightarrow$ GPT Modelle haben kein tiefes Verständnis von Texten und können daher keine Schlussfolgerungen ziehen. Antworten durch GPT basieren auf Muster im Trainingsdatensatz und generiert dadurch unlogische oder flasche Antworten.
    \item Halluzination $\rightarrow$ GPT kann die Antworten faktisch nicht überprüfen und teniert zum Halluzinieren von Informationen.
\end{itemize}

\section{Fazit}
Im nachfolgenden Abschnitt wird zunächst die aktuellen Forschungsbereiche im Bereich der Decoder Architekturen beleuchtet. Abschließend wird ein Ausblick gegeben.
\subsection{Aktuelle Forschungbereiche}
Die Entwicklung von Decoder-only Architekturen zeigte in den letzten Jahren rasante Entwicklungen.
Dennoch wird weiter an der Entwicklung von Transformer Modellen sowei Decoder Only Architekturen geforscht. Wie das Paper aus \cite{sureshSmallerFasterDecoderonly2024} zeigt, wird vor allem an der Effizienz der Transformer bzw. Decoder-only Architektur geforscht. Wie auch das GPT-3 Paper in \cite{brownLanguageModelsAre2020} zeigt, sind fähige Modelle sehr tief und haben damit einen großen Parametersatz, was das Training inneffizient und kostenintesiv macht. Das Paper aus \cite{sureshSmallerFasterDecoderonly2024} zeigt dabei Architekturalternativen (ParallelGPT, LinearGPT, ConvGPT) auf welche eine ähnliche Performance zu den GPT Modellen aufweisen.
Ein weiterer Forschungsbereich ist ob Decoder Only Modelle Turing Vollständig sind \cite{robertsHowPowerfulAre2024}. Das Paper zeigt, dass auch kleine Decoder Only Modelle Turing Vollständig sind. Damit können Decoder Only Modelle jede theoretisch berechenbare Funktion ausführen.
Weitere relevante Forschungsgebiete sind die in den GPT Papern erwähnten Biases in Sprachmodellen.

\subsection{Ausblick}
GPT und die damit verbundenen Decoder Architekturen haben in den letzten Jahren große Fortschritte in der NLP Forschung gebracht. Die Entwicklung von GPT1 bis GPT-4 zeigt die Weiterentwicklung von Decoder-only Architekturen in den letzten Jahren. Die Anwendungsbereiche von Decoder Architekturen sind dabei vielfältig. Die aktuellen Forschungen zeigen, das vor allem die Modellgröße ein relevanter Faktor für die Güte von Modellen ist, daher gibt es berechtigte Bestrebungen an der Effizienz von Decoder Architekturen bzw. großen Sprachmodellen generell zu forschen. Ein wichtiger Aspekt bei der Verwendung von Sprachmodellen sind dabei immer der Bias der durch den vortrainings Datensatz entstehen kann. Die damit Verbundene Diskriminierung ist ein relevantes Forschungsgebiet welche in den nächsten Jahren weiter erforscht werden sollte. Es bleibt dennoch abzuwarten wie sich die Forschung im Bereich der Decoder Architekturen weiterentwickelt und welche neuen Anwendungsbereiche sich daraus ergeben.




\bibliography{AMLundXAI}

\end{document}
